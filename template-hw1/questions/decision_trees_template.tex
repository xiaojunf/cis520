%% LyX 1.6.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{babel}

\begin{document}

\section{Decision trees}
\begin{enumerate}
\item Concrete sample training data.

\begin{enumerate}
\item The sample entropy $H(Y)$ is 0.985. \begin{align*}
H(Y)= & \;-\sum_{y}p(Y=y)log_{2}p(Y=y)\\
= & \;-p(Y=+)log_{2}p(Y=+)-p(Y=-)log_{2}p(Y=-)\\
= & \;\frac{4}{7}log_{2}(\frac{7}{4})+\frac{3}{7}log_{2}(\frac{7}{3})\\
= & \;0.985\end{align*}

\item The information gains are $IG(X_{1})=0.183$ and $IG(X_{2})=0.045$.
\begin{align*}
IG(X_{1})= & \; H(Y)-H(Y|X_{1})\\
= & \; H(Y)+\sum_{y,x}p(Y=y,X_{1}=x)log_{2}p(Y=y|X_{1}=x)\\
= & \; H(Y)+\sum_{y,x}p(Y=y,X_{1}=x)log_{2}\frac{p(Y=y,X_{1}=x)}{p(X_{1}=x)}\\
= & \;0.985+\frac{1}{3}log_{2}\frac{7}{8}+\frac{5}{21}log_{2}\frac{5}{13}+\frac{1}{21}log_{2}\frac{1}{8}+\frac{8}{21}log_{2}\frac{8}{13}\\
= & \;0.183\\
IG(X_{2})= & \; H(Y)-H(Y|X_{2})\\
= & \; H(Y)+\sum_{y,x}p(Y=y,X_{2}=x)log_{2}p(Y=y|X_{2}=x)\\
= & \;0.985+\frac{1}{3}log_{2}\frac{7}{10}+\frac{5}{21}log_{2}\frac{5}{11}+\frac{1}{7}log_{2}\frac{3}{10}+\frac{2}{7}log_{2}\frac{6}{11}\\
= & \;0.045\end{align*}

\item The decision tree that would be learned is shown in Figure \ref{fig:decision_tree}.
%% The [H], in combination with the float package, forces latex to
 %% generate the figure in exactly this part of the document
 %% instead of ``floating'' it to another part.
 %
\begin{figure}[H]
 \centering \tikzstyle{dir}={[}->, very thick{]} \begin{tikzpicture}{[}auto{]}
\node{[}rectangle{]} (root) at (0,0) {root}; \node (left) at (-2,-2)
{$\ldots$}; \node (right) at (2,-2) {$\ldots$};

\draw{[}dir{]} (root) to {[}above{]} node {} (left); \draw{[}dir{]}
(root) to {[}above{]} node {} (right); \end{tikzpicture} \caption{The decision tree that would be learned.}


\label{fig:decision_tree} 
\end{figure}

\end{enumerate}
\item Proof that $IG(x,y)=H[x]-H[x\mid y]=H[y]-H[y\mid x]$, starting from
the definition in terms of KL-divergence: \begin{align*}
IG(x,y)= & \; KL\left(p(x,y)||p(x)p(y)\right)\\
= & \;\sum_{x}\sum_{y}p(x,y)log(\frac{p(x)p(y)}{p(x,y)})\\
= & \;\sum_{x}\sum_{y}p(x,y)logp(x)-\sum_{x}\sum_{y}p(x,y)logp(x|y)\\
= & \;\sum_{x}p(x)logp(x)-\sum_{x}\sum_{y}p(x,y)logp(x|y)\\
= & \; H[x]-H[x\mid y]\end{align*}
 
\end{enumerate}

\end{document}
