#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\end_header

\begin_body

\begin_layout Section
Fitting distributions with KL divergence
\end_layout

\begin_layout Enumerate
KL divergence for Gaussians.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The KL divergence between two univariate Gaussians is given by 
\begin_inset Formula $f=\frac{1}{2}(x-\mu_{2})^{2}-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}$
\end_inset

 and 
\begin_inset Formula $g=-log\sigma$
\end_inset

.
 
\begin_inset Formula \begin{align*}
KL(p(x)||q(x))= & \;\mathbf{E}_{p}[log\frac{\frac{1}{\sqrt{2\pi\sigma^{2}}}exp\{-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}\}}{\frac{1}{\sqrt{2\pi}}exp\{-\frac{1}{2}(x-\mu_{2})^{2}\}}]\\
= & \;\mathbf{E}_{p}[-log\sigma+\frac{1}{2}(x-\mu_{2})^{2}-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}]\\
= & \;\mathbf{E}_{p}[-log\sigma]+\mathbf{E}_{p}[\frac{1}{2}(x-\mu_{2})^{2}-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}]\\
= & \;\mathbf{E}_{p}[\frac{1}{2}(x-\mu_{2})^{2}-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}]-log\sigma\\
= & \;\mathbf{E}_{p}[f(x,\mu_{1},\mu_{2},\sigma)]+g(\sigma)\end{align*}

\end_inset

 
\end_layout

\begin_layout Enumerate
The value 
\begin_inset Formula $\mu_{1}=\ldots$
\end_inset

 minimizes 
\begin_inset Formula $KL(p(x)||q(x))$
\end_inset

.
 
\begin_inset Formula \begin{align*}
0= & \;\frac{\partial KL(p(x)||q(x))}{\partial\mu_{1}}\\
0= & \;\frac{\partial}{\partial\mu_{1}}\mathbf{E}_{p}[\frac{1}{2}(x-\mu_{2})^{2}-\frac{1}{2\sigma^{2}}(x-\mu_{1})^{2}]\\
0= & \;\frac{\partial}{\partial\mu_{1}}\mathbf{E}_{p}[\frac{1}{2}x^{2}-x\mu_{2}+\frac{1}{2}\mu_{2}^{2}-\frac{1}{2\sigma^{2}}x^{2}+\frac{x\mu_{1}}{\sigma^{2}}-\frac{1}{2\sigma^{2}}\mu_{1}^{2}]\\
0= & \;\frac{\partial}{\partial\mu_{1}}((\frac{1}{2}-\frac{1}{2\sigma^{2}})\mathbf{E}_{p}[x^{2}]+(\frac{\mu_{1}}{\sigma^{2}}-\mu_{2})\mathbf{E}_{p}[x]+\frac{1}{2}\mu_{2}^{2}-\frac{1}{2\sigma^{2}}\mu_{1}^{2})\\
0= & \;\frac{\partial}{\partial\mu_{1}}((\frac{1}{2}-\frac{1}{2\sigma^{2}})(\mu_{1}^{2}+\sigma^{2})+(\frac{\mu_{1}}{\sigma^{2}}-\mu_{2})\mu_{1}+\frac{1}{2}\mu_{2}^{2}-\frac{1}{2\sigma^{2}}\mu_{1}^{2})\\
0= & \;1\mu_{1}-\mu_{2}\\
\mu_{1}= & \;\mu_{2}\end{align*}

\end_inset

 
\end_layout

\end_deeper
\begin_layout Enumerate
KL divergence for Multinomials.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The KL divergence between two Multinomials is: 
\begin_inset Formula $KL(p(x)||q(x))=\sum_{i\; odd}$
\end_inset

.
\end_layout

\begin_layout Enumerate
The values 
\begin_inset Formula $\alpha=\ldots$
\end_inset

 and 
\begin_inset Formula $\beta=\ldots$
\end_inset

 minimize 
\begin_inset Formula $KL(p(x)||q(x))$
\end_inset

.
 
\begin_inset Formula \begin{align*}
\textrm{Lagrangian}\;\;\mathcal{L}= & \;\ldots\\
= & \;\ldots\end{align*}

\end_inset

 
\end_layout

\end_deeper
\end_body
\end_document
